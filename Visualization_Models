#1: Odds Ratios for Predictor Variables

import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv(r"C:\Users\twarr\Downloads\College_Data (1).csv")

# Create a binary target variable for graduation rate > 62.3%
data['target'] = (data['Grad.Rate'] > 62.3).astype(int)

# Selecting predictor variables
predictors = ['PhD', 'S.F.Ratio', 'Expend']

# Add a constant for the intercept in the logistic regression
X = sm.add_constant(data[predictors])
y = data['target']

# Fit the logistic regression model
model = sm.Logit(y, X)
result = model.fit()

# Calculate the odds ratios and their 95% confidence intervals
odds_ratios = pd.DataFrame({
    'Variable': predictors,
    'Odds Ratio': np.exp(result.params[1:]),  # Skip the constant
    'Lower CI': np.exp(result.conf_int().loc[:, 0][1:]),  # Skip the constant
    'Upper CI': np.exp(result.conf_int().loc[:, 1][1:])   # Skip the constant
})

# Display odds ratios
print(odds_ratios)

# Visualization of odds ratios
plt.figure(figsize=(10, 6))
sns.barplot(x='Odds Ratio', y='Variable', data=odds_ratios, orient='h', color='teal')
plt.axvline(x=1, linestyle='--', color='red')  # Reference line for an odds ratio of 1
plt.title('Odds Ratios for Predicting Graduation Rate > 62.3%')
plt.xlabel('Odds Ratio')
plt.ylabel('Predictor Variables')
plt.show()






#2: Predicted Probability Curves
import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler

# Load the dataset
data = pd.read_csv(r"C:\Users\twarr\Downloads\College_Data (1).csv")

# Create a binary target variable for graduation rate > 62.3%
data['target'] = (data['Grad.Rate'] > 62.3).astype(int)

# Selecting predictor variables
predictors = ['PhD', 'S.F.Ratio', 'Expend']
X = data[predictors]
y = data['target']

# Standardize the predictors to avoid overflow
scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=predictors)

# Add a constant for the intercept in the logistic regression
X_scaled = sm.add_constant(X_scaled)

# Fit the logistic regression model
model = sm.Logit(y, X_scaled)
result = model.fit()

# Function to generate predicted probabilities for a range of a predictor
def plot_predicted_probability(predictor, X_scaled, result, original_data):
    # Create a range of standardized values for the predictor
    min_val = X_scaled[predictor].min()
    max_val = X_scaled[predictor].max()
    values = np.linspace(min_val, max_val, 100)
    
    # Create a DataFrame to hold constant values and varying predictor
    pred_df = pd.DataFrame({predictor: values})
    
    # Fill other predictors with their median values (in standardized form)
    for var in X_scaled.columns:
        if var != predictor and var != 'const':
            pred_df[var] = X_scaled[var].median()
    
    # Add constant term for prediction
    pred_df['const'] = 1.0
    
    # Generate predicted probabilities
    pred_probs = result.predict(pred_df)
    
    # Plot predicted probabilities
    plt.figure(figsize=(8, 6))
    plt.plot(values, pred_probs, color='teal', linewidth=2)
    plt.title(f'Predicted Probability of Graduation Rate > 62.3% vs {predictor} (Standardized)')
    plt.xlabel(f'{predictor} (Standardized)')
    plt.ylabel('Predicted Probability')
    plt.ylim(0, 1)
    plt.grid(True)
    plt.show()

# Plot predicted probability curves for each predictor
for predictor in predictors:
    plot_predicted_probability(predictor, X_scaled, result, data)






#3: Confusion Matrix Visualization
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv(r"C:\Users\twarr\Downloads\College_Data (1).csv")

# Create a binary target variable for graduation rate > 62.3%
data['target'] = (data['Grad.Rate'] > 62.3).astype(int)

# Selecting predictor variables including categorical 'Private' variable
predictors = [
    'Private', 'Apps', 'Top10perc', 'F.Undergrad', 'P.Undergrad', 
    'Outstate', 'Room.Board', 'Books', 'Personal', 'PhD', 
    'Terminal', 'S.F.Ratio', 'perc.alumni', 'Expend'
]
X = data[predictors]
y = data['target']

# Convert categorical variable 'Private' to numeric (0 for No, 1 for Yes) using .loc
X.loc[:, 'Private'] = X['Private'].apply(lambda x: 1 if x == 'Yes' else 0)

# Standardize the numerical predictors
scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=predictors)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# Fit the logistic regression model
log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)

# Make predictions on the test set
y_pred = log_reg.predict(X_test)

# Generate the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
class_names = ['<= 62.3%', '> 62.3%']  # Class labels for visualization

# Visualization of the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.title('Confusion Matrix for Logistic Regression Model')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Display classification report
print(classification_report(y_test, y_pred, target_names=class_names))
